[project]
name = "llm_engineering"
version = "0.1.0"
description = ""
authors = [
    { name = "iusztinpaul", email = "p.b.iusztin@gmail.com" },
    { name = "Bryan \"Beege\" Berry" }
]
requires-python = ">=3.11,<3.12"
readme = "README.md"
license = "MIT"
dependencies = [
    "zenml[server]==0.90.0",
    "pymongo>=4.15.1,<5",
    "click>=8.2.1,<9",
    "loguru>=0.7.3,<0.8",
    "rich>=14.1.0,<15",
    "numpy>=2.3.3,<3",
    "poethepoet==0.37.0",
    "datasets>=4.1.1,<5",
    "torch>=2.8.0,<3",
    "requests>=2.32.0,<3",
    "selenium>=4.35.0,<5",
    "webdriver-manager>=4.0.2,<5",
    "beautifulsoup4>=4.13.5,<5",
    "html2text>=2025.4.15,<2026",
    "jmespath>=1.0.1,<2",
    "chromedriver-autoinstaller>=0.6.4,<0.7",
    "qdrant-client>=1.15.0,<2",
    "langchain>=0.3.27,<0.4",
    "sentence-transformers>=5.1.1,<6",
    "langchain-openai>=0.3.33,<0.4",
    "jinja2>=3.1.6,<4",
    "tiktoken>=0.11.0,<0.12",
    "fake-useragent>=2.2.0,<3",
    "langchain-community>=0.3.30,<0.4",
    "fastapi>=0.115.8,<=0.130",
    "uvicorn>=0.37.0,<0.38",
    "opik>=1.8.57,<2",
]

[dependency-groups]
dev = [
    "ruff>=0.13.2,<0.14",
    "pre-commit>=4.3.0,<5",
    "pytest>=8.4.2,<9",
]
openstack = [
    "s3fs>=2025.9.0",
    "kubernetes>=33.1.0,<34",
    "openstack-config>=1.0.2,<2",
]

[tool.uv]
default-groups = [
    "dev",
    "openstack",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.poe.tasks]
# Data pipelines
run-digital-data-etl-beege = "uv run python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_beege.yaml"
run-digital-data-etl = [
    "run-digital-data-etl-beege",
]
run-feature-engineering-pipeline = "uv run python -m tools.run --no-cache --run-feature-engineering"
run-generate-instruct-datasets-pipeline = "uv run python -m tools.run --no-cache --run-generate-instruct-datasets"
run-generate-preference-datasets-pipeline = "uv run python -m tools.run --no-cache --run-generate-preference-datasets"
run-end-to-end-data-pipeline = "uv run python -m tools.run --no-cache --run-end-to-end-data"

# Utility pipelines
run-export-artifact-to-json-pipeline = "uv run python -m tools.run --no-cache --run-export-artifact-to-json"
run-export-data-warehouse-to-json = "uv run python -m tools.data_warehouse --export-raw-data"
run-import-data-warehouse-from-json = "uv run python -m tools.data_warehouse --import-raw-data"

# Training pipelines
run-training-pipeline = "uv run python -m tools.run --no-cache --run-training"
run-evaluation-pipeline = "uv run python -m tools.run --no-cache --run-evaluation"

# Inference
call-rag-retrieval-module = "uv run python -m tools.rag"

run-inference-ml-service = "uv run uvicorn tools.ml_service:app --host 0.0.0.0 --port 8000 --reload"
call-inference-ml-service = "curl -X POST 'http://127.0.0.1:8000/rag' -H 'Content-Type: application/json' -d '{\"query\": \"My name is Paul Iusztin. Could you draft a LinkedIn post discussing RAG systems? I am particularly interested in how RAG works and how it is integrated with vector DBs and LLMs.\"}'"

# Infrastructure
## Local infrastructure
local-docker-parent-image-build = "docker buildx build --platform linux/amd64 -t localhost:5000/zenml-base:latest -t registry:5000/zenml-base:latest ."
local-docker-parent-image-push = "docker push localhost:5000/zenml-base:latest"
local-docker-infrastructure-up = "docker compose up -d"
local-docker-infrastructure-down = "docker compose stop"
local-docker-infrastructure-delete-containers = "docker compose down"
local-docker-infrastructure-delete = [
    "local-docker-infrastructure-delete-containers",
    "local-docker-infrastructure-delete-volumes"
]
local-docker-parent-image-prepare = [
    "local-docker-parent-image-build",
    "local-docker-parent-image-push"
]

## Settings
export-settings-to-zenml = "uv run python -m tools.run --export-settings"
delete-settings-zenml = "uv run zenml secret delete settings"

## SageMaker
create-sagemaker-role = "uv run python -m llm_engineering.infrastructure.aws.roles.create_sagemaker_role"
create-sagemaker-execution-role = "uv run python -m llm_engineering.infrastructure.aws.roles.create_execution_role"
deploy-inference-endpoint = "uv run python -m llm_engineering.infrastructure.aws.deploy.huggingface.run"
test-sagemaker-endpoint = "uv run python -m llm_engineering.model.inference.test"
delete-inference-endpoint = "uv run python -m llm_engineering.infrastructure.aws.deploy.delete_sagemaker_endpoint"

## Docker
build-docker-image = "docker buildx build --platform linux/amd64 -t llmtwin -f Dockerfile ."
run-docker-end-to-end-data-pipeline = "docker run --rm --network host --shm-size=2g --env-file .env llmtwin uv poe --no-cache --run-end-to-end-data"
bash-docker-container = "docker run --rm -it --network host --env-file .env llmtwin bash"

# QA
lint-check = "uv run ruff check ."
format-check = "uv run ruff format --check ."
lint-check-docker = "sh -c 'docker run --rm -i hadolint/hadolint < Dockerfile'"
gitleaks-check = "docker run -v .:/src zricethezav/gitleaks:latest dir /src/llm_engineering"
lint-fix = "uv run ruff check --fix ."
format-fix = "uv run ruff format ."

[tool.poe.tasks.local-docker-infrastructure-delete-volumes]
shell = "docker volume ls -q --filter 'name=llm-engineers-handbook_*' | xargs docker volume rm"

[tool.poe.tasks.test]
cmd = "uv run pytest tests/"
env = { ENV_FILE = ".env.testing" }
